---
layout: post
title: "Sarsa"
date:   2019-10-25
author: "TerryRen"
header-img: "img/post-bg-infinity.jpg"
header-mask: 0.3
mathjax: true
tags:
  - ReinforcementLearning

---
on-policy

可以把它比喻成一个顽固的实践派：人家的经验我不要，我只相信自己的行动经验，人家的间接经验、包括本agent以前episode的经验教训，我全不理会。要是撞南墙。。。。那就撞把。

SARSA算法根Q-learning很像，也是基于Q-table，但是不同的是，在每一个episode的每一个step，我们会确定下一步采取的动作，而不是在下一个step开始时动态的确定step。



Sarsa公式如下：


$$
Q(S,A) \leftarrow Q(S,A)+\alpha[R+\gamma Q(S',A')-Q(S,A)]
$$

Sarsa算法中，Q(S,A)更新方式中，行为全部都是agent本次episode执行的行为。这意味着，以前曾经完成过的episode，这里完全不理会、不知道、不参考。